# 2023-12-08 商周十二月專欄訪談唐鳳部長逐字稿

### 江殷年：
首先想請教部長，7 月時 OpenAI 推出「超智慧對齊」團隊，希望可以幫助所有人，把 AI 校正到符合人類預期的目標。

不知道對部長來說，覺得這可行嗎？可以促進 AI 安全嗎？

### 唐鳳：
你們已經有固定的中譯了嗎？Superalignment 你們已經翻成「超智慧對齊」了？

### 江殷年：
對，我們上一期刊出的內容是這樣沒錯。

### 唐鳳：
OK。對我來講，它的意思比較像「自動對齊」，也就是先從半自動，然後慢慢到全自動的對齊。這種對齊方法，並沒有說它只能用在所謂的超智慧，因為超智慧定義每個人不一樣。

精確地來講，它是讓這一代的 AI，去幫忙對齊下一代的 AI。所以這不是什麼全新的東西，已經進行一陣子了。

### 江殷年：
因為我不算很了解 AI，想問部長您所謂的「對齊」是指什麼？因為我知道有另外一個名詞叫「微調」，不知道這兩種的差別在哪？

### 唐鳳：
製作生成式 AI 系統，基本上有三個基本步驟：預訓練、微調、部署。

「預訓練」就像是你拿一堆教科書、文字或影像等，把它壓縮成一個模型。因為輸入的資料非常多，壓縮出來的量非常小，但是你又希望它能夠解壓縮到全部的資料。這個按照定義是不可能沒有損失的，就是壓縮率有個極限。那超過某個極限之後，機器學習的特色，就是它會發明新的壓縮法。

早期網路上面有些 JPEG 圖檔，當太複雜的圖檔，要用很小的頻寬傳輸的時候，壓縮後的顏色會混在一起；或者字的邊緣開始出現雜訊。同樣的，影片如果傳輸的頻寬不夠，好比像說我們現在在視訊，如果忽然間頻寬不足，你也會看到我有點抽格，臉變成一格一格的，或者是有毛邊等等。

所以在這個情況下，生成式 AI 的特色是，它會去找出新的壓縮法，好比它會注意到人的臉大概都長這個樣子，然後它就不用記住所有你臉上像素的顏色，它只要記得你特徵的地方的顏色，在解壓縮的時候，重新畫一張人臉出來就好。

在這個情況下，它畫出來的，有的時候就是幻覺、無中生有。也就是本來壓縮資料裡面有的東西，在它歸納特徵之後，覺得絕大部分都有這種特徵，但是你在問它問題的時候，因為它沒有完整的原始資料，所以就還原成它想像出來的樣子。如果你跟 ChatGPT 聊過，你會發現說，它可能講得頭頭是道，甚至說誰誰誰哪一年說了什麼，可是根本沒有這件事情，這就是它的幻覺。

所以預訓練的時候，它只能回答出它輸入的那一大堆資料裡面，最有可能的那種模式。這是第一步。

「微調」是指我們告訴它什麼樣的答案才是好的，什麼樣的答案我們不想接受。舉例來說，不誠實的答案是不好的，誠實的比較好，意思是如果訓練資料裡沒有，乾脆說沒有，不要無中生有一些東西。又好比損害人的是不好的，關懷人的是好的。

好比就算預訓練資料裡面，有很多可以用來做奸犯科、製造全世界流行的生物病毒，或製造癱瘓全世界網路的那種電腦病毒等等，但是不要這樣回答，因為會損害跟你對話的人類。反之，模型應該勸勸他說，不要問這種問題之類的。這是指AI無害的部分。

當然除了誠實、無害之外，還是多多少少要對人有點幫助。因為達到誠實、無害是最簡單的方法，也就是每個問題都回答「無可奉告」，這樣不會傷害你，但也沒有什麼意義。所以在誠實、無害的前提下，多少還是透露一點預訓練資料裡的東西給人類，這樣比較有幫助。

所以大概就是誠實、無害、有幫助，這三個是微調的重點。所以要把預訓練的模型微調，往往不是增強它的能力，相反地可能會減少它的能力，減少它用在不誠實、有害、沒幫助的地方。

### 江殷年：
我有聽過一個說法，所謂「縮小」，這樣會減少它的參數嗎？還是不是這樣子？

### 唐鳳：
有很多產出微調的方法。有個方法叫做 LoRa，可以想成一個濾鏡，就是本來模型在這裡，原本打算輸出這個結果，但是微調出某個接在它後面的處理器，來改變這個結果。這樣就不是它的答案直接給你，而是經過這一層過濾再給你。在篩選過後，或者是調整過後，它本來要這樣講的，但是把裡面可能會造成損害的部分改掉。

當然也可以在微調時把它合併回去，讓整個模型變成經過微調的模型，有各種技術上的方法，但概念上把它想成濾鏡，是比較容易的。

### 江殷年：
這樣的話，跟超智慧對齊的差別又是什麼？

### 唐鳳：
「超智慧對齊」在問的是整個微調的過程，能不能主要讓上一代的 AI 來做，而不是讓人類來做？

好比，你是智慧法庭的法官，碰到很複雜的專利侵權案，法官未必是那個領域的專家。但我們在法院或仲裁案上面用的方法，並不是要法官變成該領域的大師或者是專家，這是不可能的。相反的，你是讓指控的那一方跟防禦的那一方，就是原告跟被告，雙方都請最好的專利律師來，然後讓他們在那邊辯論給你聽。

這時，雙方都會提出很多證據，每次開庭的時候都會再提出一批新的證據，說上一次攻防的時候，對方講的其實沒有道理，因為這邊有 prior art、有證據等等。所以，到最後法官需要做的判斷，就不是關於這個案子全部細節的判斷，主要是關於確保這兩方辯論品質的判斷。

運用這個模式，法官需要做的事就變少了，因為他不需要是那個領域的專家，他只需要是這種辯論格式的專家。這樣大概可以了解？

### 江殷年：
可以。對！

### 唐鳳：
這種辯論攻防，就是所謂超智慧對齊的主要做法之一。我不用去偵測這回答本身是好或壞，但是我讓某個 AI 來試圖說服我說這是壞的、這有損害等等；另一個 AI 這邊要主張說其實沒有，這是好的等等。

只要能預先建立這種辯論的格式，人類甚至只需要抽測答辯攻防的文字紀錄就好。

### 江殷年：
所以 OpenAI 的團隊才會說，我們不需要用人類來訓練 AI，用 AI 來訓練 AI 才是更聰明的方式。是因為這個理由嗎？

### 唐鳳：
是的。當 AI 複雜到某一個程度，只靠人類來訓練 AI，人類這邊會需要花太多的時間，雖然理論上做得到，就像理論上法官也可以花十年變成那個領域的專家，只是沒有必要這樣做。

### 江殷年：
了解，那我可以不可以說，如果 OpenAI 真的做出了超智慧對齊，譬如說像我可能不是 AI 方面的專家，我不是專才，可是我可以少掉十年的時間？靠著這個，我就可以成為專家嗎？可以這麼說嗎？

### 唐鳳：
我覺得跳得有點快。先回頭一下下，像我剛剛講到那些非常抽象的原則，例如「無害」等等，要更落地一點的話，我先分享另一個概念，叫做「憲法式 AI」。

「憲法式 AI」的意思就是說，想辦法把「無害」兩個字，把它展開成更具體的原則。舉例來講，像聯合國《世界人權宣言》，裡面就把無害展開，成為「每個人都不能對別人施以殘忍的、不人道或侮辱性的待遇」，或是像「法律之前人人平等」，這個大家都會背了。

這樣在訓練 AI 的時候，就可以說不只是爭辯無害還是有害，而是可以爭辯你有沒有合乎《世界人權宣言》的某項原則，這就跟法庭上的攻防越來越像了。按照這個憲法，有訂定一些後面的作用法等等，如果是判例法國家，還包括過去的具體情況等等，如此一來，兩個 AI 在辯論的時候，就可以引經據典，依照這份憲法文件或衍生文件的任何一行來說，這個回答犯法、這個回答違憲；另外一個AI就說沒有，可以看前面這個原則，這樣講是可以的，因為我是在描述創意的情境，不是真的將特定人類怎麼樣等等的答辯。

所以人類可以省10年的工，因為我們的工作就變成來判斷這份憲法是不是足夠好。如果你覺得訓練出來的模型，沒有達到某個精神，只要回來調整憲法的文字就可以了，這個任何人都可以做。好比：你看《聯合國人權宣言》，你也可能想說再加一行也不錯吧？任何人都做得到，這時對齊的參與門檻就大幅下降，人類要花的時間也大幅減少。

### 江殷年：
了解。所以我們回過來剛剛跳太快的部分，應該是說我可以藉由 OpenAI 的這種機制，我本來不是專家，但是我可以用他們跑出來的東西，我來判斷，可以這麼說嗎？

### 唐鳳：
你在判斷完之後，覺得這個真的不行、不符合需求，你也只需要用一般的語言來告訴它。這有點像是你不需要是法律專家，你只要講說在這種情況下，我覺得這個判斷原則沒有符合我的需求，我覺得用什麼別的原則會更好，只要這樣講就好，模型就可以重新再微調一次，讓它更符合你的需求。

### 江殷年：
哇，那這樣子，部長覺得「超智慧對齊」對人類接下來的意義或者是幫助是什麼？因為聽起來非常厲害，好像可以省掉其他人以前原本 10 年的時間，馬上就可以有這個功力，馬上就看到這些東西了？

### 唐鳳：
而且這不用等未來，像 [TAIDE](https://taide.tw/)就是國科會訓練的對話引擎，也已經用了憲法式 AI 的做法來調整。

今年我們數位部在臺南沙崙跟臺北各辦了一場審議式工作坊，也就是點子松的「對齊大會」。工作坊邀請到在關心 AI 如何跟社會互動的人，我們先發一個互動式的問卷，每個人都可以填寫問卷題目，然後收攏成大家都覺得重要的某些基本原則。之後，我們再面對面，透過工作坊小組討論，確保這些基本原則，在桌上的人可以接受，最後就綜整出各 10 條左右的基本原則。

接下來，我們就請 TAIDE 團隊對他們已經訓練到一個程度的模型，灌進這份憲法文件，確保這樣出來的模型，更符合我們當地的需求。如果沒有這樣子做，因為他們預訓練的材料，是過去幾十年的新聞資料等等，那時候的時空環境跟現在已經不完全一樣。所以 TAIDE 預設的回答，雖然很接近它的訓練材料，從我們今天的角度來看，已經不一定都完全適合了。

透過憲法式的訓練，TAIDE 模型可以學到我們今天臺灣社會對它的期待，跟它預訓練資料年代裡的期待，有什麼不一樣。所以我的回答是，如果每個人都可以參與設定對齊標的，就有一個好處，就是整個 AI 模型，就比較不會是少數人來決定絕大多數人的日常的互動。

因為我們日常會用越來越多的 AI，目前的情況是，我如果看到它不是我想要的狀態，我很難立刻去調整它。就算透過一些 Prompt Engineering 等等的方式去調整提示詞，但它後面的那些我難以察覺的偏見，還是沒有辦法系統性的去調整。

也就是說，目前並不是每個人都可以為 OpenAI 訓練出 GPT 的 LoRa 濾鏡，但是如果我們更加強化對齊的技術，使之越來越成熟，確實可以加速這天的到來，就是每個人都可以快速地跟自己、跟旁邊的夥伴，只要共筆一份憲法文件或蒐集判例文件，我們也可以訓練出一個小 AI，讓這個小 AI 來對齊某個大 AI，這個大 AI 對齊完之後，就是我們所要的樣子。

### 江殷年：
了解。OpenAI 說他們想用 AI 來確保實行預期的目標，可是我有疑問，這裡我們可以理解成「OpenAI 預期的目標」，但它符合全人類的權益嗎？還是只符合他們自己的權益、他們股東的權益？如果只遵守這個的話，是不是會很危險？

### 唐鳳：
如果某個 AI 模型，一直出現幻覺、回答也充滿損害，那沒有投資人會想投資的。事實上，去年 ChatGPT 3.5 幻覺程度高到這樣，但大家還是很喜歡，我有點訝異。以前我協助蘋果 Siri team 六年的時間，從 2010 到 2016。當時 Siri 跟 Wolfram Alpha 這些系統整合的時候，兩邊都最注重的，就是不要出現幻覺，不要變成 Siri 聽到你一個問題去 Wolfram Alpha 回來，結果文不對題、回答造成損害之類，所以，非常準確、非常無害，這是低標，沒有這個不行的。

但顯然去年的 ChatGPT 有個很好的切入點，就是它就算有幻覺、就算亂講，但它並不會堅持己見。也就是說，你只要告訴它說「你錯了」，它立刻道歉，即便是非常非常巴結。所以這個情況下，人類好像在這種情境，比較能夠容忍它胡說八道，反正最後我糾正它，它不會抗拒。

但就如同你說的，把 ChatGPT 做成這個樣子，是對 OpenAI 好沒有錯，特別對OpenAI的市值好。但釋出這種東西，對全人類真的好嗎？這個是一個大問號。其實就連 Superalignment team 目前的負責人 Jan Leike 也說，他回頭想一想，如果再晚一點推出 GPT、等它更對齊一點再部署的話，可能對世界比較好。

但是誰來做這種決定？這是很好的問題。也就是說，有沒有可能因為競爭的關係，所以就預先釋出了還沒有那麼對齊的 AI，只是因為擔心競爭者也會這樣子做？

這就好像沙場上遍佈地雷，然後雙方要開車離開危險的戰區，但地雷如果一爆炸，就會連鎖全部一起爆炸。你會想說我已經開得很穩定，但還是得衝快一點、趕快脫身。為什麼？因為別人說不定沒有那麼穩定，他就先衝過去了。

這是一個很微妙的賽局，就是如果跑得太快，當然大家一起爆炸，但是如果跑得慢，就會想說我跑這麼慢是不是更危險？因為大家都會擔心在自己看不到的地方，有人正在衝很快，所以都不會自動停下來。所以這是一個問題，就是你要確保對齊到什麼程度，才把它跟社會銜接？這是第一個問題。

第二個是說我們在做這種超對齊的時候，它如果有偏見，是不是只處理這個公司所在的文化、法律管轄領域在意的偏見，其他是不是就算了、就不管了？反正這是對美國公司、對美國主流文化有利的，那其他少數語言或者少數文化，是不是忽略掉也無所謂？這是第二個問題，bias 的問題。

我們目前的方向是這樣：第一個，在 bias 這邊有另外一些人，好比像 Meta 的 Llama、微軟的 Orca，他們主張我們就把某些顯然還沒有強到會毀滅社會、預訓練過的模型，根本不做對齊，就直接公開出來。意思是任何人都可以拿到一開始沒對齊的版本，然後接著微調，所以結果就是大家都很平均地學會了微調技術，這時如果我們這邊覺得被忽略了，那我們接著訓練就好了。

事實上，這就是 TAIDE 對 Meta 的 Llama 做的事情。就是 Llama 訓練到一個程度，TAIDE 接過來繼續訓練，這樣多多少少可以解決你剛剛問的，是不是 Meta 出了模型，只對 Meta 好？不盡然如此，因為別人可以接著訓練，所以可能訓練到某個對 Meta 不好的方向，Meta反正也不管你，Meta唯一管你的就是你不能一次給七億人使用，不能成為它的商業競爭對手，但如果只給 2,300 萬人使用，Meta 不會管你。這是不同的策略，確實有助於解決 bias 的問題。到這裡沒有問題？

### 江殷年：
沒有問題，但 OpenAI 不是開源的，那要怎麼解決？

### 唐鳳：
先回答要怎麼解決 bias。只要整個研究圈有產出這種公開可見的，對齊、debias 的機制，好比我是冰島政府的話，我就可以跟 OpenAI 說：「你看，事實上有這樣子的，把模型往冰島的文化園對齊的能力。而且我按照公開資料，也把 Meta 的 Llama 對齊到一個程度了。」那為什麼 OpenAI 不這樣做？即便我也可以吸收一部分成本，為什麼不合作做這件事情呢？如果 OpenAI 持續不這樣子做，在這點上會有比OpenAI更好的競品，消費者就會改去用競品就好了。這是簡單的市場機制。

所以這個時候，OpenAI 也必須投資一定的程度，用民主的方法來蒐集當地的偏好。事實上 OpenAI 也有資助十組團隊做這件事情，叫 Democratic Inputs。而且 OpenAI 也有跟我們的夥伴 [CIP.org](https://cip.org/)合作，問全美國具有統計代表性的 1,000 人有什麼期待，然後對此做出回應，所以 OpenAI 在市場力量驅動下，也開始往這些方向走。但是確實如你說的，因為OpenAI並不是 Open Source，所以他們自己能這樣做，別人比較不容易幫忙，我們能夠做的是營造出一種市場環境，讓OpenAI了解到說，如果完全不讓大家幫你針對特定文化對齊、去除偏見，那其實市場上很快就會有競品，也許你的產品未來就比較賣不動。這個部分 OK 嗎？

### 江殷年：
OK，所以我可以說，就是 OpenAI 雖然不是開源的，可是如果想要贏過別人的話，其實市場力量會幫助他們越沒有偏見越好。可以這麼說嗎？

### 唐鳳：
這有點像過去冰箱用的冷媒（氟氯碳化物）會破壞臭氧層，後來大家都發現這件事，所以大家都坐下來約定說，不是明天就完全不用，畢竟替代方案還沒有完全發明，但是在未來的某一天，按照《蒙特婁協定》，未來某一天再用，就是公害了。這樣講就有一個很明確的標準，到未來的某一天，如果還用舊的方法，就再也拿不到投資，也接不到生意了，這有點像現在的綠色金融概念。這時，就會產生很強的誘因，讓下一輪的注資者得到信號說，如果再不用某種方式去蒐集大家偏好，並且迅速對齊的話，這個領域是不會有未來的，它就是公害了。

到這點上，未來的商業決策者，就會把這一點放到他們的考量。這就是我們叫做 Race to AI Safety，朝向 AI 安全的競賽；並不是你衝得比較慢，而是你換個方向，找到沒地雷的方向去跑。

### 江殷年：
我們可不可以總結說：不管是 OpenAI 的超智慧對齊，或是 Meta 那種開源的對齊，其實都會更加促進 AI 的安全和開放，會嗎？還是有一些挑戰？

### 唐鳳：
可以這樣理解，當然也有一些挑戰。

好比：模型的能力強大到某個程度，要經過多少紅隊演練，試著讓它做壞事、確定都沒有問題，才把它放給不同人來使用？要等多久才釋出？因為如果不解決這個問題， OpenAI 或其他的 lab裡面的人真的都找不出太大漏洞，就釋出了。現在民間都有對齊技術，對齊是往一群人或是某個個人的偏好對齊，但世界上也存在某些人，他的工作就是用不法的方式，去損害別人謀取自己的利益，也有這種人。當然如果現在世界上完全沒有這種人，也許很多安全問題不用討論，但事實上是有這種人。

所以，當他發現一個比目前能力強大的 AI 出現的時候，他也可以試圖用他的小 AI，來濫用這個大 AI。舉例來講，他以前可能是透過一對一的資訊操弄方式，去說服你趕快去買什麼虛擬資產，把錢全部都匯到他戶頭裡。他可能已經訓練出某個小模型，專門用來做資訊操弄。以前攻擊方需要至少跟你講相同的語言，而且對文化有一定的了解，才能成功說服你。現在假設他能運用某個更強的 AI、更了解你的語言跟文化，他自己和他的小 AI 雖然不會你的語言，但是透過大模型來操弄就好了。所以我們怎麼樣去偵測說，它不會被這樣子誤用？

或者，如果已經被這樣誤用的話，人類文明要怎樣迅速了解，在接下來有這樣能力的模型要進入我們的生活了，怎麼做額外的調整？好比匯款上限的限制，或者若要冒充公部門的話，我們都是用 111 等簡碼才當真的公部門，其他都不認。透過這些調整做完之後，才能釋放出來。這個如果沒做好，就會變成是 lab 用他們有限的經驗覺得沒問題，然後就部署了。

我再舉一個例子，不法集團也可能透過這個模型，去誘使它駭入它自己機房的系統，然後把整個模型的權重重弄，其實它才幾百 Gb 的一個檔案而已，把這個檔案放到不法集團拿得到的地方。也就是說，OpenAI 本來沒有釋出模型，結果透過一些攻擊的方式，說服 GPT 把自己偷到外面去，然後就可以進一步控制了，這也是另外一種攻擊方法。或者，我透過這個模型去說服 OpenAI 的員工，或者在訓練資料裡滲透了犯罪集團的指令，結果就可以把他偷出來等等，這些都是部署前要考慮的風險，也就是你把一個 AI 模型接到世界，讓全世界都可以跟他講話時，要預防的東西。

只有等這些都預防到一個程度，對齊才可以發揮完整的作用，不然的話，又會變成魔高一丈的情況。

### 江殷年：
我回到前面，部長有講說憲法式的 AI，那這樣是不是代表憲法式的 AI 其實很重要？

### 唐鳳：
對。我們只要確保說絕大部分的人，都是至少對《人權宣言》應該不排斥的。如果每個人都有類似強度的 AI 的話，那我想絕大部分的人，是會願意把他的那個強度的 AI，調整成至少不違背最基本的憲法式原則。

這個時候雖然也許會有些犯罪集團等等，但大家隨時守望相助的能力也提高了，就不會變成是只靠少數的人來偵測，而是每個人都可以偵測、都可以想出因應的方式。本來文明就是這樣子，也就是大家越了解某種威脅時，每個人都可以想出因應這些威脅的方式這樣子。

### 江殷年：
我發現我有一點小誤會，原來不是把《世界人權宣言》灌進去訓練OpenAI，然後我再部署，其他人其實就沒有辦法去更改？

還是您的意思是說，我直接釋出模型給某個人，而他已經有對齊技術了，就可以用《世界人權宣言》的概念去對齊，是嗎？

### 唐鳳：
這兩種都可以，首先可能是某個 lab，它已經自己對齊到一個很強的程度，以至於你要透過互動來教壞它幾乎不可能，這是一個情況。

另外的可能性是說，它對齊到某個還不錯的程度，但還是可能有些漏洞，接著再邀請別的 lab、政府機構，包括我們 AI 評測中心，去試圖找出額外的漏洞，並且提出額外的對齊策略來。在這個流程裡面，就變成有三股力量：做預訓練和初始對齊的；做後續紅隊演練的；做進一步對齊的。這些都完成之後，才是終端的使用者。

這也是很有可能的，因為畢竟前面這個階段的 lab，它組成的成員，並沒有多元到可以預期到對每個文化、每個社會都不造成損害的高標準。

### 江殷年：
了解。最後一個問題，我蠻好奇的，因為剛剛部長有講到說，現在 AI 的賽局，各家都在競爭，然後這週 Google 也有推出 Gemini，但是他們有說不是為了打敗 GPT。

我蠻好奇說，部長怎麼看現在的這個賽局的狀況？因為現在演變成 Google 表示他們想要打的是生態系之戰，那 OpenAI 應該要怎麼樣？現在是不是真的沒有其他的選擇？

### 唐鳳：
我們文章不要用地雷的比喻好了，因為地雷畢竟有點 sensitive。我們改一個比喻。

想像這些 labs 是一群人，面前是結冰的湖面，他們都很想要到彼岸，就是他們想要的、或他們投資人想要的東西在對面。結冰的湖面有個風險，就是如果你踩上去的話，你會掉下去，而且那個冰層的裂縫會擴大，所以不只你掉下去，最後也有可能大家都掉下去。

所以有一些情況，確實只有你掉下去，那就是商業判斷，踩下去會股價腰斬，這畢竟只是一家公司的事。更嚴重一點的漏洞如果真的踩到了，像剛剛講合成全世界都流行的傳染病，或者合成網際網路癱瘓的電腦病毒等等，當然人類文明應該還是會恢復，但是大家日子都很不好過。當然也有更嚴重的破洞，就是你踩下去所有人都一起溺死，也沒有剩誰了，那種就是所謂生存風險，這種破洞也許比較少，但如果踩下去你只能踩一次，下次就沒有人類了。

在這個情況下，如果彼此不溝通，大家是不會自動停下來的，因為你總會覺得總有某人正在跑，那還不如我先跑到彼岸、到了堅實的陸地上，我還可以救人。這是很自然的想法，但問題就是每個人都容易高估自己的能力。所以好比說一排10個人，總有某個人高估了自己的能力，然後大家都掉下去，造成悲劇。

要改變這種賽局就是要靠協調，也就是說每個人都要有基本的假設，就是如果看到可能的漏洞要通知其他人。這是很關鍵的基本假設，就是我不會等別人掉下去，我只要看到任何人快要踩到漏洞，我就會通知所有人，而且我們要盡可能的投資在不是跑更快，而是找到更沒有漏洞的那個方向的裝備上，手電筒、掃描儀這些。

所以這不是說不投資，也不是說不跑，而是像 OpenAI 提出 20% 的算力，投資在這種找漏洞的方向，讓大家看清楚眼前好幾步，而不是只有眼前半步的那種技術上，也就是我們今天在討論的「超智慧對齊」技術。

因為現在做安全跟對齊的研究員的人數，在這些 lab 裡，比起努力跑越來越快、做效能的研究的人，兩個的比例大概是 1 比 30。也就是說，有 30 個人在往前衝，只有一個在那邊調整方向，所以事實上非常不對稱。如果大家可以同意說，我們投資讓更多人願意加入對齊的工作，至少變成 1 比 1 的話，那踩下去漏洞的機率至少可以降低，因為你每一步都在掃描前面好幾步。這是第一個。

第二個，大家就要有共識說，我看東西可能有盲點，所以我也願意其他人來看我正在走的方向，這樣大家看到漏洞可能性也變大了，我們要彼此看顧。這個情況下大家跑的速度，可能都稍微慢一點點，但到最後大家都會往安全的方向跑，就比較不會掉下去，概念上是這樣。

目前的情況還不完全是這樣。我們可以看到 Google DeepMind、Anthropic、OpenAI、Microsoft 等等，現在組成 Frontier Model Forum，基本上就是在做我剛剛講的那兩件事：彼此之間互相通報，以及投注更多的資源來研究對齊跟安全。

問題就是說，不是只有他們在研究這些 AI。目前還沒有所有研究者都同意的，共同對齊、協調的方法。

### 江殷年：
了解，所以現在我可以這樣說嗎？就是你越早推出越安全、越對齊的 AI，你就會跑在市場的前面。

### 唐鳳：
沒錯，然後投資人也好，消費者也好，就會願意鼓勵你繼續往這個方向發展。

### 江殷年：
所以，我們的 AI 安全其實在某種程度上，是靠市場驅動的？

### 唐鳳：
是。作為政策制定者，我們的工作就是去確保，透過政府的投資以及政策對市場給出訊號：我們在意什麼東西、接下來什麼東西會變成是重要的，甚至會變成基本要求。這是我們可以對市場發出的訊號。

所以像我們最近剛開幕的評測中心，就發了這十個訊號，說接下來任何模型，從原型開始，這十件事情我們會覺得是重要的：安全、彈性、準確、當責、隱私、可解釋、公平、透明、可靠、資安。如果在這十件事情上背道而馳，這就是一個訊號說，雖然現在沒有要禁用，但我們不鼓勵市場往這些方向發展。

### 江殷年：
但是，大家依舊在用的話，會不會其實幫助有限？

### 唐鳳：
應該這樣講，就是當你有兩個能力相當的選擇，有一個犯錯時你調整他有用，第二個犯錯時不當責、完全沒有辦法對那種情況負責，這時你當然選願意當責的那個，假設他們能力一樣。現在大家擔心的狀況是，你要做出取捨：也就是當責、可解釋的那個，能力非常弱；而胡說八道、無法解釋的，反而能力比較強。這個時候就會造成困難。

比如，如果我是高度規管的行業，像律師之類的，我就沒有那個動機去用亂講的，但是如果我是犯罪集團，我有很強的動機去用那個亂講的。在這個情況下，整個社會就會開始往魔高一丈的方向發展。

我們能給出來的訊號就是，首先，我們透過像評測中心這樣的機制，給我一個模型，我來告訴你它負責的程度，這第一個。第二個是，我們會盡可能地把我們政府的投資方向，或者是透過外交，協助全世界來轉向到更準確、更當責而不犧牲太多能力的這些產品。這些產品出現的時候，市場上的消費者，包含高度規範行業裡面的人，就有好的選擇可以選了。一旦他們有好的選擇，他們的投資和消費，就不會流向相對不好的方向。

所以這裡面有兩個要點：一個是要很明確地說，那台冰箱會破壞臭氧層、這台不會。第二個是說，這台不會的，它的製冷效率也沒有差多少，所以大家趕快去買這個。只要這兩個元素都存在，市場就會往比較好的方向走。

### 江殷年：
我有一個最後的問題：這樣說的話，如果要簡化成想要做不法行為的人其實根本就不受管制，他們就是因為要做不法行為，所以一直使用那個 AI，這樣政府或是政策有辦法去管制他們嗎？或者是這其實對某一些 AI 來說，這些罪犯是他們的客戶，這就變成新的商機？他們一樣會有新的訂閱戶去訂閱他，有一些支持？

### 唐鳳：
這也是很好的問題。當然以 OpenAI 的角度來看，他們也不想變成到最後只有罪犯在使用的技術。先前有些新興技術，因為很多洗錢、資助恐怖分子都採用，所以那種技術的市場就受到影響。虛擬資產技術當然有很多好的用途，但因為不法之徒找到某些規避監管的方式，所以你也可以看到他們正在轉向，朝著 web3 是基礎建設、研發對大家都好的公共財，這類的方向在發展。因為他們也發現，如果這些技術被認為主要是犯罪集團使用，那市場是不認同的。

簡單來說，目前當然有使用 AI 技術犯罪的人。但是，我們要有一個很強的訊號，是這個技術可以往安全的方向發展，也就是往抵抗攻擊能力、增強防禦能力的方向發展。只要有這個訊號，那就不會整個行業都被當成壞人，簡單來講是這樣子。

我舉一個蠻實際的例子，像之前大家都提到 deepfake，影像或者是聲音什麼，現在都可以簡單的仿冒。解決這個問題的方式並不是禁止所有人研發這個技術，而是說我們很明確的說，如果用它來散播未經同意的私密影像，或者是用它來做違反金融消費者保護的線上詐騙，或者是在接近選舉的時候冒充候選人意圖使他不當選。在這三件事情上，立法院都已經通過說，我們不是禁止這個技術，但是你用在這三件事情上，那就是犯罪了。

也就是說，明確的在它造成社會損害之前，你要畫一個標誌出來說，你跑車不管開多快，到這邊就是違規了。這樣市場就會知道我投資這些技術的時候，至少在臺灣這三件事情是違法的，不要往這個方向發展。

### 江殷年：
了解，謝謝部長，這就是我所有的問題。部長還有其他想補充的觀點嗎？包括 AI 評測中心或是點子松之類的問題？

### 唐鳳：
對，我想有兩個，一個是說 AI 評測中心評測的項目這 10 個是大原則，但底下具體的項目，我們會透過不斷的滾動檢討，大家覺得我們評測需要加什麼項目，或者是現在 AI 造成的新的損害是我們現在無法了解到的，我們希望有個機制，讓我們合作的這些研究機構、社群的夥伴們告訴我們說還有哪些要評測的，這是所謂民主化的對齊。

也就是說，對齊的這 10 個目標，裡面的細項，甚至是第 11 個、第 12 個原則，如果有的話，都可以透過即時的方式來回饋給我們。AI 評測中心，並不是現在明確的說我們未來一年只評測這些，而是會從社會繼續收集意見，運用類似審議式工作坊的那種方式，來收集我們要評測什麼。這是第一個。

第二個，我還是強調說「超智慧對齊」，雖然 OpenAI 是取這個名字，但是它嚴格來講，其實就是從半自動慢慢到全自動對齊、減少人類在這裡需要消耗的力氣，又可以更好地達到對齊的目標。所以，它並不是未來的、科幻的技術，而是每天都已經在使用的技術。OpenAI 的這個研究團隊，是在強化它的自動化程度，但他們並不是從頭創一門學門。這些做法，包含剛剛講到的審議、自我辯論等等，其他的 lab 裡也有，而學術界也有人做這樣子的研究。

所以，這有點像是當時我不是很想一直講「Metaverse」，因為不想幫特定公司打廣告，是一樣的道理。所以雖然 Superalignment 這個 branding 只有 OpenAI 在用，但這件事其實大家都在做。

### 江殷年：
好，了解，那今天的訪問應該差不多就這樣。謝謝部長，謝謝。
